#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DART ìë™ ë‹¤ìš´ë¡œë” v2 (ê²€ìƒ‰ ê°œì„ : ì •í™•ì¼ì¹˜ + ë¶€ë¶„ì¼ì¹˜ ë™ì‹œ í‘œì¶œ)
----------------------------------------------------------------
- "ì‚¼ì„±"ì²˜ëŸ¼ ì§§ì€ í‚¤ì›Œë“œë„ ê³„ì—´ì‚¬ê°€ ì—¬ëŸ¬ ê°œ ë‚˜ì˜¤ë„ë¡ ìˆ˜ì •
- ì •í™•ì¼ì¹˜ê°€ ìˆìœ¼ë©´ ë§¨ ìœ„ì—, ê·¸ ì™¸ ë¶€ë¶„ì¼ì¹˜ë„ í•¨ê»˜ ë‚˜ì—´
- ìµœëŒ€ 200ê±´ í‘œì‹œ, ìƒì¥ì‚¬(ì£¼ì‹ì½”ë“œ ìˆìŒ) ìš°ì„  ì •ë ¬

ì„¤ì¹˜:
    pip install requests pandas openpyxl
"""

import io
import re
import time
import zipfile
import requests
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

API_HOST = "https://opendart.fss.or.kr"
CORPCODE_API = f"{API_HOST}/api/corpCode.xml"
LIST_API    = f"{API_HOST}/api/list.json"
DOC_API     = f"{API_HOST}/api/document.xml"

CACHE_DIR = Path("_dart_cache")
CACHE_DIR.mkdir(exist_ok=True)

S = requests.Session()
S.headers.update({"User-Agent": "dart-auto-downloader/1.2"})

def sanitize_filename(name: str) -> str:
    if not name:
        name = "unknown_report"
    bad = r'\\/:*?"<>|'
    for ch in bad:
        name = name.replace(ch, "_")
    name = "_".join(name.split())
    return name[:120]

def input_nonempty(prompt: str) -> str:
    while True:
        s = input(prompt).strip()
        if s:
            return s

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_corp_master(api_key: str) -> pd.DataFrame:
    cache_zip = CACHE_DIR / "corpCode.zip"
    cache_xml = CACHE_DIR / "CORPCODE.xml"
    need = True
    if cache_xml.exists():
        if (time.time() - cache_xml.stat().st_mtime) < 30*24*3600:
            need = False
    if need:
        print("ğŸ“¥ ë²•ì¸ì½”ë“œ ë§ˆìŠ¤í„° ë‹¤ìš´ë¡œë“œ ì¤‘â€¦")
        r = S.get(CORPCODE_API, params={"crtfc_key": api_key}, timeout=60)
        r.raise_for_status()
        cache_zip.write_bytes(r.content)
        with zipfile.ZipFile(io.BytesIO(r.content)) as zf:
            zf.extractall(CACHE_DIR)

    import xml.etree.ElementTree as ET
    root = ET.parse(cache_xml).getroot()
    rows = []
    for el in root.findall(".//list"):
        rows.append({
            "corp_code": el.findtext("corp_code") or "",
            "corp_name": el.findtext("corp_name") or "",
            "stock_code": el.findtext("stock_code") or "",
        })
    return pd.DataFrame(rows)

def search_companies(master: pd.DataFrame, query: str) -> pd.DataFrame:
    """íšŒì‚¬ëª… ê²€ìƒ‰ (ì •í™•ì¼ì¹˜ + ë¶€ë¶„ì¼ì¹˜ ë™ì‹œ í‘œì¶œ)"""
    q = (query or "").strip()
    if not q:
        return master.head(0)

    m = master.copy()
    m["__norm"] = m["corp_name"].fillna("").str.replace(r"\s+", "", regex=True)
    qn = re.sub(r"\s+", "", q)

    # ì •í™•ì¼ì¹˜(ê³µë°±/ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    mask_exact = m["__norm"].str.casefold() == qn.casefold()
    exact = m[mask_exact].copy()
    exact["__rank"] = 0  # ìµœìš°ì„ 

    # ë¶€ë¶„ì¼ì¹˜
    mask_part = m["__norm"].str.contains(re.escape(qn), case=False, regex=True)
    part = m[mask_part & (~mask_exact)].copy()
    part["__rank"] = 1

    # í•©ì¹˜ê¸°: ì •í™•ì¼ì¹˜ ë¨¼ì €, ê·¸ ë‹¤ìŒ ë¶€ë¶„ì¼ì¹˜
    res = pd.concat([exact, part], ignore_index=True)

    # ì •ë ¬: rank â†’ ìƒì¥ì‚¬ ìš°ì„ (stock_code ë¹„ì–´ìˆì§€ ì•ŠìŒ) â†’ íšŒì‚¬ëª…
    res["__listed"] = res["stock_code"].fillna("").ne("")
    res = res.sort_values(by=["__rank", "__listed", "corp_name"], ascending=[True, False, True])

    return res.head(200).drop(columns=["__norm","__rank","__listed"], errors="ignore")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_list(api_key: str, corp_code: str, year: str) -> List[Dict]:
    out = []
    page_no = 1
    bgn_de = f"{year}0101"
    end_de = f"{year}1231"
    print(f"ğŸ“¡ ê³µì‹œ ëª©ë¡ ì¡°íšŒ: {corp_code} / {bgn_de} ~ {end_de}")
    while True:
        params = {
            "crtfc_key": api_key,
            "corp_code": corp_code,
            "bgn_de": bgn_de,
            "end_de": end_de,
            "page_no": page_no,
            "page_count": 100
        }
        r = S.get(LIST_API, params=params, timeout=60)
        r.raise_for_status()
        data = r.json()
        if data.get("status") != "000":
            print("âŒ list.json ì˜¤ë¥˜:", data.get("message"))
            break
        items = data.get("list") or []
        out.extend(items)
        total = int(data.get("total_count", 0))
        if len(out) >= total or not items:
            break
        page_no += 1
        time.sleep(0.15)
    print(f"âœ… ëª©ë¡ {len(out)}ê±´ ìˆ˜ì§‘")
    return out

def is_zip(content: bytes) -> bool:
    return len(content) > 1000 and content[:4] == b"PK\x03\x04"

def download_zip(api_key: str, rcept_no: str, out_dir: Path, rcept_dt: str, report_nm: str) -> Optional[Path]:
    """ZIP ë‹¤ìš´ë¡œë“œ (íŒŒì¼ëª…ì— ì œì¶œì¼+ë³´ê³ ì„œëª… í¬í•¨)"""
    params = {"crtfc_key": api_key, "rcept_no": rcept_no}
    r = S.get(DOC_API, params=params, timeout=60)
    content = r.content or b""
    if r.status_code == 200 and is_zip(content):
        # íŒŒì¼ëª… êµ¬ì„±: 20241021_ì‚¬ì—…ë³´ê³ ì„œ_20241021001234.zip
        safe_name = sanitize_filename(f"{rcept_dt}_{report_nm}_{rcept_no}")
        out = out_dir / f"{safe_name}.zip"
        out.write_bytes(content)
        return out
    return None

def main():
    print("\n=== DART ìë™ ë‹¤ìš´ë¡œë” v2 (ê²€ìƒ‰ ê°œì„ ) ===\n")
    api_key = input_nonempty("ğŸ”‘ OpenDART API Key: ")

    master = fetch_corp_master(api_key)
    while True:
        keyword = input_nonempty("\nğŸ¢ íšŒì‚¬ëª…(ë¶€ë¶„ ì¼ì¹˜ ê°€ëŠ¥): ")
        cand = search_companies(master, keyword)
        if cand.empty:
            print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.")
            continue

        cand = cand.reset_index(drop=True)
        print(f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼ {len(cand)}ê±´ (ìµœëŒ€ 200ê±´ í‘œì‹œ)")
        for i, row in cand.head(200).iterrows():
            sc = f" / ì£¼ì‹ì½”ë“œ:{row['stock_code']}" if row["stock_code"] else ""
            print(f"  [{i}] {row['corp_name']}  (corp_code:{row['corp_code']}{sc})")

        sel = input("ì„ íƒí•  ë²ˆí˜¸ ì…ë ¥ (ë‹¤ì‹œ ê²€ìƒ‰í•˜ë ¤ë©´ 'r'): ").strip()
        if sel.lower() == 'r' or sel == "":
            continue
        if not sel.isdigit() or int(sel) < 0 or int(sel) >= len(cand):
            print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            continue
        pick = cand.iloc[int(sel)]
        corp_code = pick["corp_code"]
        corp_name = pick["corp_name"]
        break

    year = input_nonempty("\nğŸ“… ë‹¤ìš´ë¡œë“œ ì—°ë„ (ì˜ˆ: 2024): ")
    if not (len(year) == 4 and year.isdigit()):
        print("âš ï¸ ì—°ë„ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ í˜„ì¬ ì—°ë„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        year = str(datetime.now().year)

    safe_corp = sanitize_filename(corp_name)
    out_dir = Path(f"D:/DART_ê³µì‹œìë£Œ/DART_{year}_{safe_corp}_{corp_code}_ZIP")
    out_dir.mkdir(exist_ok=True)
    print(f"\nğŸ“ ì €ì¥ í´ë”: {out_dir.resolve()}")

    items = fetch_list(api_key, corp_code, year)
    if not items:
        print("ì¢…ë£Œí•©ë‹ˆë‹¤. (ëª©ë¡ ì—†ìŒ)")
        return

    summary = []
    for idx, it in enumerate(items, start=1):
        rcept_no  = it.get("rcept_no", "")
        rcept_dt  = it.get("rcept_dt", "")
        report_nm = it.get("report_nm") or it.get("rpt_nm") or "unknown_report"
        file_label = f"{rcept_dt}_{sanitize_filename(report_nm)}.zip"

        print(f"[{idx}/{len(items)}] {report_nm} ({rcept_dt})  ì ‘ìˆ˜ë²ˆí˜¸:{rcept_no}")
        existing = out_dir / f"{rcept_no}.zip"
        if existing.exists() and existing.stat().st_size > 1000:
            print("  â†ª ì´ë¯¸ ì¡´ì¬ (ê±´ë„ˆëœ€)")
        else:
            out = download_zip(api_key, rcept_no, out_dir, rcept_dt, report_nm)
            if out:
                print(f"  âœ… ì €ì¥: {out.name}  â†’ í‘œì‹œìš©: {file_label}")
            else:
                print(f"  âš ï¸ ì‹¤íŒ¨ (document.xml ì‘ë‹µì´ ZIPì´ ì•„ë‹˜)")

        summary.append({
            "ê¸°ì—…ëª…": corp_name,
            "corp_code": corp_code,
            "ë³´ê³ ì„œëª…": report_nm,
            "ì ‘ìˆ˜ë²ˆí˜¸": rcept_no,
            "ì œì¶œì¼": rcept_dt,
            "ZIPì €ì¥íŒŒì¼": out.name if out else f"{rcept_no}.zip",
            "DARTë§í¬": f"https://dart.fss.or.kr/dsaf001/main.do?rcpNo={rcept_no}"
        })

        time.sleep(0.35)

    df = pd.DataFrame(summary)
    excel_path = out_dir / f"ê³µì‹œZIPìš”ì•½_{year}.xlsx"
    csv_path   = out_dir / f"ê³µì‹œZIPìš”ì•½_{year}.csv"
    try:
        df.to_excel(excel_path, index=False)
    except Exception as e:
        print("  âš ï¸ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨:", e)
    try:
        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    except Exception as e:
        print("  âš ï¸ CSV ì €ì¥ ì‹¤íŒ¨:", e)

    print("\nğŸ‰ ì™„ë£Œ!")
    print(f"ğŸ“Š ìš”ì•½(Excel): {excel_path}")
    print(f"ğŸ“„ ìš”ì•½(CSV)  : {csv_path}")
    print(f"ğŸ§¾ ZIP ê°œìˆ˜   : {len(df)} (í´ë” ë‚´ rcept_no.zip í˜•íƒœ)")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
